I would like to thank all three reviewers for their careful review and helpful feedback.Especially I would like to thank reviewer 2 for providing valuable references of related work. I found these both highly relevant and interesting and I do wish I would have found them before. I read and added those and others I found meanwhile to the bibliography. Also I added a discussion of the related work to a revised manuscript and pointed out common results as well as new aspects in my results that I couldnâ€™t find in those references.
An updated version can be found here https://github.com/felixbiessmann/fipi/blob/poltext/manuscript/political_bias_prediction.pdf; 
related work is discussed in the new section 2.I do hope that the reviewers would consider taking this into account.I would like to address first the three main concerns of the reviewers and then answer some specific concerns in more detail.1) Missing related work:I did extensive literature research, compared and discussed as many studies I could cover and added them to the revised manuscript. My results are well in line with previous findings but extend existing work with new analyses. 2) The analysis only covers german.That is mostly due to the availability of data - but I will look into more languages asap.3) Low and badly explained accuracy of predictions:I compared the results with previous similar studies (Yu et al, "Classifying party affiliation from political speech", 2008 and Hirst et al, "Text to Ideology or Text to Party Status?", 2014).My results are on par with or better than the results reported in those studies on comparable data. Importantly also the explanations and empirical tests of these accuracies provided in my manuscript were in line with the findings in those studies:a) the results of binary party classification (see Yu et al, 2008) was comparable (accuracies between 0.8 and 0.87) to the accuracy in the present study (0.85 for the 17th Bundestag)b) the accuracy in the five class classification of the 17th Bundestag (0.63) is slightly better than the classification accuracy of 0.61 which Hirst et al obtained on a five class problem of political affiliation prediction in the European parliament.c) As demonstrated in the my submission the main sources of poor accuracy are the out-of-domain generalization challenge and the short length of the texts. Both effects were described before in Yu et al, 2008 and Hirst et al 2014, respectively, in similar (but not the same) settings.Also it would be worth noting that even if the accuracies are low (as those of comparable studies), they nonetheless are on average above chance. While these results do not allow for usage in production systems for classification, it is well possible to use such a system as assistive technology for human annotators in an active learning setting for priorization of to-be-labeled data. MORE DETAILED COMMENTSREVIEWER 2Q: [...] other classifiersA: I did try others (random forests, kernel based methods); results on in-domain data were slightly better but results on out of domain data were significantly worse (despite regularization).Q: What about stemming of words?A: I did include this in earlier hyperparameter optimizations, but it did not improve results, so i excluded it for brevity.Q: [sentence level too fine, topic too coarse ]A: I tried paragraphs for manifestoproject data earlier; results were between sentence and topic performance. Hirst et al, 2014 show a comprehensive analysis of text length and accuracy, though.REVIEWER 3Q: each sentence [...] associated to many labels?A: No.Q: [...] 3-fold cross-validation sufficent [...]A: [...] A single random split into train/hyperparameter-optimization/evaluation is common practice (see e.g. Netflix prize); I did 5 fold nested CV, results were the same.Q: d is the size of the dictionary [...] union of the words [...] in the corpus and the words [...] in the sentiment dictionaryA: YesQ: the sentiment vector s includes all sentiment-related words, both positive and negative.A: It contains the sentiment polarity values associated with the key words.Q: What is N in Table 1?A: Number of data points, the revised manuscript includes this.Q: [...] difference in performance between the 17th and 18th bundestagA: one is a 5 class the other a 4 class problem, also the 18th Bundestag is still running, so the amount of data is different.Q: [...] experiment on correlations between words and parties raisesA: The experiments on word correlations and sentiment correlations are separate and independent experiments and are discussed independently.Q: [remark on table 9]A: The notion of sentiment used in the manuscript has polarity.