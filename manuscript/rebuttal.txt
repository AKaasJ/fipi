I would like to thank all three reviewers for their careful and helpful feedback.Especially I would like to thank reviewer 2 for providing valuable references of related work. I found these both highly relevant and interesting and I do wish I would have found them before. I read and added those and others I found meanwhile to the bibliography. Also I added a discussion of the related work to the revised manuscript and pointed out common results as well as new aspects in my results that I couldnâ€™t find in those references. I do hope that the reviewers would consider taking this into account.I would like to address first the three main concerns of the reviewers and then answer some specific concerns in more detail.1) Missing related work:I did extensive literature research, compared and discussed as many studies I could cover and added them to the revised manuscript.2) The analysis only covers german.That is partly due to the availability of data, but also because I didn't feel comfortable with other languages - but I will definitely look into more languages asap.3) Low and badly explained accuracy of predictions:Thanks to the references provided by reviewer 2, I was able to compare the results with previous studies (Yu et al, "Classifying party affiliation from political speech", 2008 and Hirst et al, "Text to Ideology or Text to Party Status?", 2014).I found my results to be at least on par with or better than those reported in previous studies (on comparable data sources). Importantly also the explanations and empirical tests thereof provided in my submitted manuscript were in line and in parts extended the findings in those studies. The relevant parts in the manuscript now reflect these points.a) the results of binary party classification (in the Canadian parliament, see Yu et al, 2008) was comparable (accuracies just above 0.8) to the accuracy in the present study (0.85 for the 17th Bundestag)b) the results of 0.63 accuracy on the five class classification on the 17th Bundestag is actually slightly better than the only comparable study I found: in the European parliament, Hirst et al, 2014 obtained a classification accuracy of 0.61 on a five class problem of political affiliation prediction.c) The main sources of poor accuracy are the out-of-domain generalization challenge and the short length of the texts, as demonstrated in the submission. Both effects were described before in Yu et al, 2008 and Hirst et al 2014, respectively. Also the results on the correlation between political power and speech sentiment complement the findings of Hirst et al on ideology-unspecific language in Canadian parliament debates.DETAILLED COMMENTSREVIEWER 2Q: [...] other classifiersA: I did try others (random forests, kernel based methods), but they did not improve the results.Q: What about stemming of words?A: I did include this in the hyperparameter optimization, but it did not improve results, so i excluded it for brevity.Q: [sentence level too fine, topic too coarse ]A: I tried paragraphs for manifestoproject data earlier, results were between sentence and topic performance. Hirst et al, 2014 show a great analysis of text length and accuracy.REVIEWER 3Q: each sentence [...] associated to many labels?A: No.Q: [...] 3-fold cross-validation sufficent [...]A: [...] One split into train/hyperparameter-optimization/evaluation is common practice (see e.g. Netflix prize); i did 5 fold nested CV, results were the same.Q: d is the size of the dictionary [...] union of the words [...] in the corpus and the words [...] in the sentiment dictionaryA: YesQ: the sentiment vector s includes all sentiment-related words, both positive and negative.A: It contains the sentiment polarity values associated with the key words. The manuscript now reads "was constructed from the sentiment polarity values in the sentiment dictionary." (before: "was constructed from the sentiment values in the sentiment dictionary.")Q: What is N in Table 1?A: Number of data points, the revised manuscript includes this now.Q: [remark on poor out-of-domain generalization results Table 3]A: These effects are in line with previous findings (now referenced and discussed) and I tried to explain some of the reasons with experiments the results of which are also inline with previous findings. It is just a very challenging problem and there is not a lot of data at our disposal: party policies compared to those in their manifestos _have_ to change as part of the coalition negotiations and democratic processes in the parliaments.  But the manuscript shows how some of these prediction failures can be related to the policy changes.Q: [...] difference in performance between the 17th and 18th bundestagA: one is a 5 class the other a 4 class problem, also the 18th Bundestag is still running, so the amount of data is different. The revised manuscript reflects these differences better.Q: [remark on topic level results in Table 3 and Table 5]A: These experiments were mostly meant to show the effect of text length, but the results nonetheless show that in some (not all) cases highly accurate predictions are possible.Q: [...] performance [...] of political views is not [...] high. [...] based on what this may be considered relevant and/or usefulA: The performance is significantly above chance for some (not all) classes (chance level being 1/56=0.018 assuming balanced classes). Even if the results are not perfect the model can be still highly relevant for assistive technology e.g. for speeding up human annotations in active learning scenarios.Q: [...] experiment on correlations between words and parties raisesA: The experiments on word correlations and sentiment correlations are separate and independent experiments. They are also discussed independently.Q: [remark on table 9]A: The notion of sentiment used in the manuscript has polarity. The revised manuscript reflects this better.